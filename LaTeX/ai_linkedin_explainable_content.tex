%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Explainable AI : LinkedIn Posts}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Is AI inside?}
If your software claims to be powered by AI (Artificial Intelligence) ,
then it should:

If none of the above is true, then think twice before claiming!!

Agree?


(Note: Assuming currently popular usage of the term 'AI' here and not software like rule-based expert systems which are not primarily data driven.)

				\begin{itemize}
				\item be trained on data (no data $==$ no AI-ML).
				\item improve as more data is fed, automatically.
				\item be predictive \& autonomous as much as possible.
				\end{itemize}

\#fakeai \#ai \#aimarketing \#hype \#aihype \#ml \#artificialintelligence \#software \#data \#machinelearning \#aiml \#datascience \#explainableai

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Machine Learning on Graphs}

Graphs, though omnipresent, had shied away from being in Machine Learning domain, in the past.

The reason is, they are different. They are unstructured. Unlike popular tabular (fixed rows and columns) and images (fixed length, width, and channels) data, graphs have variable number of nodes and edges, making them unsuitable as an input to Machine Learning (ML) algorithms. ML needs fixed-dimensioned-vectorized-features. Now, various approaches are getting devised to embed/vectorize graphs as well.

Say, for images, a window (convolution, filter) of fixed size runs over the image, condenses information in a fixed size format to be sent forward for ML. Now in graphs, as each node can have different number of neighbors, a special way of convolution has been devised to condense surround/neighborâ€™s information. Such, condensed/vectored information of nodes then goes on to build overall graph information (vectors, features, embeddings).

Following articles, at a wonderful site called 'distill.pub', explain Graphs and Convolutions on them.

				\begin{itemize}
				\item Intro: https://distill.pub/2021/gnn-intro/
				\item Convolutions: https://distill.pub/2021/understanding-gnns/
				\end{itemize}

\#machinlearning \#graphs \#neuralnetwork \#deeplearning \#neuralnetworks \#machinelearning \#data \#graphneuralnetwors \#gnns \#ml \#ai \#gcn \#graphconvoution

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Searching by shapes}

Searching by words/phrases is well established and highly popular way of finding what you want. Searching by images is also there, but how about searching by shapes? Interesting as well as challenging, right?

Came across this short article by Physna talking about this upcoming area of true 3D geometric search.

Fascinating personally for me as my initial 2 decades of professional career, along with MS+PhD, before moving to AI-ML-NLP, were in this field.

Paul Powers Jay Marshall

\#3d \#search \#similarity \#geometry

https://www.linkedin.com/pulse/2d-vs-3d-data-why-matter-your-business-physna/

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{'Soft' but 'more golden' labels}

Wonderful research on annotations for Machine Learning problems.

It suggests to label training data, softly, meaning, say in case of Sentiment Analysis, don't label samples as either 100% positive or 100% negative, because in reality its typically not so. There are shades of grey. So, annotate like 80% positive, 20% negative. With this, the claim is that, you need far less training data. Obviously the limitation is that you need more rigorous and more correct annotations.

The training data has to be 'more golden' as it is lesser so relied upon more.

Although an early research but looks promising, more so in Natural Language Processing, where interpretations are ambiguous inherently.

\#MIT \#machinelearning \#data \#training \#research \#classification \#clustering \#deeplearning \#annotation \#naturallanguageprocessing

https://www.technologyreview.com/2020/10/16/1010566/ai-machine-learning-with-tiny-data/?utm\_campaign=site\_visitor.unpaid.engagement\&utm\_source=LinkedIn\&utm\_medium=tr\_social

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{'tag-line like title'}

'3-5 lines background/intro'

Came across this wonderful article by @'first authors tag', et al at @'org tag' discussing the same topic.

enumerated point-wise actionable, insights, etc

For more details, have a look at the original article at 'website link'

@'remaining authors tags'

Such past suggestions, along with tweet-length brief summary, have been collected and open-sourced at https://github.com/yogeshhk/ExplainingAI

Intent of such posts is to share my journey of learning and explaining AI with a wider audience, thereby increasing awareness for moving AI in a safer and human-intent-aligned manner (effective altruism!!).

Please feel to send comment and new topics/articles at my email <firstnamelastname at yahoo dot com>

\#ai \#machinelearning \#datascience \#explainableai \#effectivealtruism \#topic1 \#topic2

\end{frame}